# Bug Prediction

## Motivation
Minimizing the number of bugs in software is an effort central to software engineering - faulty
code fails to fullfill the purpose it was written for, its impact ranges from sligthly
embarrassing to disastrous and dangerous, and last but not least - fixing it 
costs time and money. Resources in a 
software development lifecycle are almost always limited and therefore should be allocated to where
 they are needed most - in order to avoid bugs, they should be focused on the most fault-prone
 areas of the project. Being able to predict where such areas might be would allow more development and 
testing efforts to be allocated to the right places.

However, as noted in @DAmbros2012, reliably predicting which parts of source code are the most 
fault-prone is one of the holy-grails of software engineering. Thus it is not surprising that bug-prediction continues
 to garner a widespread research interest in software analytics, now equipped with the 
 ever-expanding toolbox of data-mining and machine learning techniques. In this survey we 
 investigate the current efforts in bug-prediction in the light of the advances in software analytics methods and focus our attention on answering the
  following research questions:

* **RQ1** What is the current state of the art in bug prediction?
    More specifically, we aim to answer the following:
    * What software or other metrics does bug prediction models rely on and how good are they?
    * What kind prediction models are predominantly used?
    * How are bug prediction models and results validated and evaluated?
* **RQ2** What is the current state of practice in bug prediction?
    * Are bug prediction techniques applied in practice and if so, how?
    * Are the current developments in the field able to provide actionable tools for developers?
* **RQ3** What are some of the open challenges and directions for future research?


## Research protocol
We started by studying the initial 6 seed papers which were selected based on domain knowledge:

| Reference    | Topic         | Summary                                   |
|-------------------|--------------|------------------------------------------|
| @Gyimothy2005      | metrics validation | performance of object-oriented metrics |
| @Catal2009review | literature review | comparison of metrics, methods, datasets |
| @Arisholm2010  | literature review | comparison of models, metrics, performance measures |
| @DAmbros2010 | BP performance | benchmark for bug prediction, evaluation of approaches using the bechmark|
| @Hall2012 | literature review | influence of model context, methods, metrics on performance  |
| @Lewis2013 | case study | BP deployment in industry |

We looked for additional papers by making searches based on the following elements:

1. Keyword search using search engines (Scopus, ACM Digital Library, IEEE Explorer).
The search query was constructed so that the paper title had to contain the phrase bug prediction,
 but also the other more general variants used in literature: *bug/defect/fault prediction*. 
The title also had to contain at least one of following keywords: *metrics*, *models*,
 *validation*, *evaluation*, *developers*. To remain within the bug prediction field we required
  *software* to appear in the abstract.

2. Filtering search results by publication date. We narrow the scope to *recent* papers, which we define as "published within the last 10 years" for the purposes of this review. Therefore we exluded papers published before 2008. 

3. Filtering by the number of citations. We selected papers with 10 or more citations in order to focus on the ones that already have some visibility within the field.

4. Exploring other impactful publications by the same authors.

The papers chosen had to fullfill the above and additional criteria which we summarise in Table 1 below.

_Table 1. Inclusion and exclusion criteria._

| Inclusion criteria                      |Exclusion criteria     |
|-----------------------------------------|-----------------------|
| Discuss software metrics, prediction models, evaluation of models | Discuss bug severity |
| cited 10 or more times | older than 10 years |




_Table 1. Papers found by investigating the authors of other papers._

| Starting point    | Type         | Result                                   |
|-------------------|--------------|------------------------------------------|
| @DAmbros2010      | is author of | @DAmbros2012                             |
| @Catal2009review  | is author of | @Catal2011 <br> @Catal2009investigating  |

## Answers

### Metrics
 To find out what metrics are commonly used to build fault prediction models, we studied the
  papers that either focus their entirety or at least a section to the discussion of software
  metrics used for bug prediction. Based on the results and observations made in the relevant
   studies, we classify the software metrics most commonly used in two groups based on which 
   aspect of a piece of software they measure:

 * *the product* - that is, the (static) code itself
 * *the process* - that is, the different aspects that describe how the product developed through
  time. We include both simpler change-log based metrics that require different versions as well 
  as metrics that require detailed process recording.

@DAmbros2010 use a slightly different vocabulary and discuss *single-version* and *change log* 
approaches to building software metrics while @Moser2008 refer to the same concepts as 
*code metrics* and *change metrics*. 

#### Code metrics
Different metrics incur different costs based on whether they require additional efforts in order 
to collect data and setup an instrumentation infrastructure. Measuring various aspects of what is 
already available - a snapshot of source code - is an obvious starting point when building a set 
of predictive features and @Catal2009review found code metrics were the most commonly studied 
metrics in literature. Traditionally, examples of metrics obtained by static analysis include 
the completely language agnostic *lines of code* (LOC) as well as *code complexity* (for example, 
McCabe and Halstead complexities), with the latter applicable for languages with a structured 
control flow and the concept of methods. @DAmbros2012 articulates a rationale behind using code 
complexity for bug prediction - if code is complex, it is difficult to change and is therefore 
fault-prone.

Another group of more language specific metrics which became popular after 1990 
(@Catal2009review) are the *object-oriented* or *class* metrics which are based on the idea of 
classes and measure class-related concepts like coupling, inheritance and cohesion. 
@Radjenovic2013, @Malhotra2015 and @Catal2009review found the metrics suite given by Chidamber 
and Kemerer (CK, see table X) to be the most common among the OO metrics and it was used in 
@Gyimothy2005 to study the predictive power of object-oriented metrics.

A drawback of static code metrics is identified in @Rahman2013 - the authors find that such 
metrics have high stasis which means they do not change a lot from release to release. In turn, 
this can cause stagnation in the prediction models, which classify the same files as 
fault-prone over and over.

#### Process metrics
The other prominent group of metrics are the process metrics which are related to the software 
developement process and the changes of the software through time. Additional infrastructure has 
to be in place in order to capture the process features and typically at least a version control 
system is required. @DAmbros2012 notes one of the rationales for using these metrics in a 
predictive model - faults are introduced by changes in software and should be studied. 

Some examples of process metrics as used in @Moser2008 and @Rahman2013 include code churn, 
number of revisions, active developer count, distinct developer count, changed code scattering, 
average lines of code added per revision and age of a file.

@Radjenovic2013 group metrics that are derived by looking at the source code change history as 
either **delta metrics** or **code churn** . Delta metrics are not separate new metrics per se, 
but are derived from other metrics by comparing versions of software to each other.

@Moser2008 compared the power of code and process metrics by analysing the Eclipse project 
and found the process metrics outperform code metrics. @Rahman2013 observed that process metrics 
perform better as predictors compared to code metrics and are more stable across releases. 
However, @DAmbros2012 note that such results should be taken with a grain of salt; when comparing 
a selection of representative bug prediction approaches, the authors found that the results 
considering metrics highly depend on the choice of learner and could not be generalised.

@Giger2011 explore potential advantages of using more fine-grained source code changes, capturing 
them at statement level and including the changes' semantics. They obtained this data by building 
abstract syntax trees and using the changes required to transform one AST to the other as 
metrics. They found their models outperformed models based on other coarser metrics such as code 
churn; however, the gain in performance comes at the cost of extracting these fine-grained 
changes.

##### Developer-based metrics
Developer-based process metrics that can be extracted from version control history are 
specifically studied in @Matsumoto2010, which found modules touched by more developers were 
likely to contain more faults and that combining developer metrics with either other process or 
code metrics improved prediction performance. @Lee2011 explore developer factors further by 
examining how developer direct interaction with software predicts faults. By tracking interaction 
events like selecting and editing source files, they found that such *micro-interaction metrics* 
significantly improve prediction accuracy. The most informative metrics proved to be the number 
of low-degree-of-interest-file editing events, the event pattern of editing and selecting 
error-prone files consecutively, and time spent on editing.
Finally, @DiNucci2018 introduce *scattering metrics*, which capture how focused a developer 
is when working on a piece of code. Structural scattering describes how structurally far apart 
in the project the code being worked on is and semantic scattering measures how different the 
code being edited is in terms of implemented responsibilities. The authors find they have 
relatively high accuracy and perform better than a baseline selection of code and process metrics. 

### Open challenges and future work
- The role of developer-related factors in the bug prediction field is still a partially explored area
- Still no strong conclusions - process metrics were found to perform better, but are more 
expensive to obtain. Then again, code metrics have high stasis and might result in stale models. 
- Defect prediction is a field where external validity is very hard to achieve
