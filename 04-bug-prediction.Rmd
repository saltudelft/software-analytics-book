# Bug Prediction

## Motivation
Minimizing the number of bugs in software is an effort central to software engineering - faulty
code fails to fulfill the purpose it was written for, its impact ranges from slightly
embarrassing to disastrous and dangerous, and last but not least - fixing it 
costs time and money. 
Resources in a software development lifecycles are almost always limited and 
therefore should be allocated to where  they are needed most - in order to avoid bugs, 
they should be focused on the most fault-prone areas of the project. 
Being able to predict where such areas might be would allow more development and 
testing efforts to be allocated on the right places.

However, as noted in @DAmbros2012, reliably predicting which parts of source code are the most 
fault-prone is one of the holy-grails of software engineering. 
Thus it is not surprising that bug-prediction continues to 
garner a widespread research interest in software analytics, 
now equipped with the ever-expanding toolbox of data-mining and machine learning techniques. 
In this survey we  investigate the current efforts in bug-prediction 
in the light of the advances in software analytics methods and 
focus our attention on answering the following research questions:

* **RQ1** What is the current state of the art in bug prediction?
    More specifically, we aim to answer the following:
    * What software or other metrics does bug prediction models rely on and how good are they?
    * What kind prediction models are predominantly used?
    * How are bug prediction models and results validated and evaluated?
* **RQ2** What is the current state of practice in bug prediction?
    * Are bug prediction techniques applied in practice and if so, how?
    * Are the current developments in the field able to provide actionable tools for developers?
* **RQ3** What are some of the open challenges and directions for future research?


## Research protocol
We started by studying the initial 6 seed papers which were selected based on domain knowledge:

| Reference    | Topic         | Summary                                   |
|-------------------|--------------|------------------------------------------|
| @Gyimothy2005      | metrics validation | performance of object-oriented metrics |
| @Catal2009review | literature review | comparison of metrics, methods, datasets |
| @Arisholm2010  | literature review | comparison of models, metrics, performance measures |
| @DAmbros2010 | BP performance | benchmark for bug prediction, 
                                  evaluation of approaches using the benchmark|
| @Hall2012 | literature review | influence of model context, methods, metrics on performance  |
| @Lewis2013 | case study | BP deployment in industry |

We looked for additional papers by making searches based on the following elements:

1. Keyword search using search engines (Scopus, ACM Digital Library, IEEE Explorer).
The search query was constructed so that the paper title had to contain the phrase bug prediction,
 but also the other more general variants used in literature: *bug/defect/fault prediction*. 
The title also had to contain at least one of following keywords: *metrics*, *models*,
 *validation*, *evaluation*, *developers*. To remain within the bug prediction field we required
  *software* to appear in the abstract.

2. Filtering search results by publication date. We narrow the scope to *recent* papers, 
   which we define as "published within the last 10 years" for the purposes of this review. 
   Therefore we excluded papers published before 2008. 

3. Filtering by the number of citations. 
   We selected papers with 10 or more citations in order to focus on 
   the ones that already have some visibility within the field.

4. Exploring other impactful publications by the same authors.

The papers chosen had to fulfill the above and additional criteria 
which we summarise in Table 1 below.

_Table 1. Inclusion and exclusion criteria._

| Inclusion criteria                      |Exclusion criteria     |
|-----------------------------------------|-----------------------|
| Discuss software metrics, prediction models, evaluation of models | Discuss bug severity |
| cited 10 or more times | older than 10 years |




_Table 2. Papers found by investigating the authors of other papers._

| Starting paper    | Relationship | Result                                   |
|-------------------|--------------|------------------------------------------|
| @DAmbros2010      | is author of | @DAmbros2012                             |
| @Catal2009review  | is author of | @Catal2011 <br> @Catal2009investigating  |

## Answers







### Metrics
 To find out what metrics are commonly used to build fault prediction models, we studied the
  papers that either focus their entirety or at least a section to the discussion of software
  metrics used for bug prediction. Based on the results and observations made in the relevant
   studies, we classify the software metrics most commonly used in two groups based on which 
   aspect of a piece of software they measure:

 * *the product* - that is, the (static) code itself
 * *the process* - that is, the different aspects that describe how the product developed through
  time. We include both simpler change-log based metrics that require different versions as well 
  as metrics that require detailed process recording.

@DAmbros2010 use a slightly different vocabulary and discuss *single-version* and *change log* 
approaches to building software metrics while @Moser2008 refer to the same concepts as 
*code metrics* and *change metrics*. 

#### Code metrics
Different metrics incur different costs based on whether they require additional efforts in order 
to collect data and setup an instrumentation infrastructure. Measuring various aspects of what is 
already available - a snapshot of source code - is an obvious starting point when building a set 
of predictive features and @Catal2009review found code metrics were the most commonly studied 
metrics in literature. Traditionally, examples of metrics obtained by static analysis include 
the completely language agnostic *lines of code* (LOC) as well as *code complexity* (for example, 
McCabe and Halstead complexities), with the latter applicable for languages with a structured 
control flow and the concept of methods. @DAmbros2012 articulates a rationale behind using code 
complexity for bug prediction - if code is complex, it is difficult to change and is therefore 
fault-prone.

Another group of more language specific metrics which became popular after 1990 
(@Catal2009review) are the *object-oriented* or *class* metrics which are based on the idea of 
classes and measure class-related concepts like coupling, inheritance and cohesion. 
@Radjenovic2013, @Malhotra2015 and @Catal2009review found the metrics suite given by Chidamber 
and Kemerer (CK, see table X) to be the most common among the OO metrics and it was used in 
@Gyimothy2005 to study the predictive power of object-oriented metrics.

A drawback of static code metrics is identified in @Rahman2013 - the authors find that such 
metrics have high stasis which means they do not change a lot from release to release. In turn, 
this can cause stagnation in the prediction models, which classify the same files as 
fault-prone over and over.

#### Process metrics
The other prominent group of metrics are the process metrics which are related to the software 
development process and the changes of the software through time. Additional infrastructure has 
to be in place in order to capture the process features and typically at least a version control 
system is required. @DAmbros2012 notes one of the rationales for using these metrics in a 
predictive model - faults are introduced by changes in software and should be studied. 

Some examples of process metrics as used in @Moser2008 and @Rahman2013 include code churn, 
number of revisions, active developer count, distinct developer count, changed code scattering, 
average lines of code added per revision and age of a file.

@Radjenovic2013 group metrics that are derived by looking at the source code change history as 
either **delta metrics** or **code churn** . Delta metrics are not separate new metrics per se, 
but are derived from other metrics by comparing versions of software to each other.

@Moser2008 compared the power of code and process metrics by analysing the Eclipse project 
and found the process metrics outperform code metrics. @Rahman2013 observed that process metrics 
perform better as predictors compared to code metrics and are more stable across releases. 
However, @DAmbros2012 note that such results should be taken with a grain of salt; when comparing 
a selection of representative bug prediction approaches, the authors found that the results 
considering metrics highly depend on the choice of learner and could not be generalised.

@Giger2011 explore potential advantages of using more fine-grained source code changes, capturing 
them at statement level and including the changes' semantics. They obtained this data by building 
abstract syntax trees and using the changes required to transform one AST to the other as 
metrics. They found their models outperformed models based on other coarser metrics such as code 
churn; however, the gain in performance comes at the cost of extracting these fine-grained 
changes.

##### Developer-based metrics
Developer-based process metrics that can be extracted from version control history are 
specifically studied in @Matsumoto2010, which found modules touched by more developers were 
likely to contain more faults and that combining developer metrics with either other process or 
code metrics improved prediction performance. @Lee2011 explore developer factors further by 
examining how developer direct interaction with software predicts faults. By tracking interaction 
events like selecting and editing source files, they found that such *micro-interaction metrics* 
significantly improve prediction accuracy. The most informative metrics proved to be the number 
of low-degree-of-interest-file editing events, the event pattern of editing and selecting 
error-prone files consecutively, and time spent on editing.
Finally, @DiNucci2018 introduce *scattering metrics*, which capture how focused a developer 
is when working on a piece of code. Structural scattering describes how structurally far apart 
in the project the code being worked on is and semantic scattering measures how different the 
code being edited is in terms of implemented responsibilities. The authors find they have 
relatively high accuracy and perform better than a baseline selection of code and process metrics. 








### Models
In the section below we will show different models presented by several studies.

#### Static predictors
Many older bug-prediction methods [insert refs] use the static code for bug prediction,
these methods argue that complex code is more difficult to change and 
therefor introduces more bugs.

#### Code changes
Predictors based on code changes argue that code changes induce bugs.
And as it turns out, methods based on code modifications clearly outperform
static prediction methods @Moser2008.

##### History Complexity Metric (HCM) @hassan2009
Files that are modified during periods of high change complexity will contain more faults.
Developers changing code during these periods will make mistakes,
because they probably will not be aware of the current state of the code.

Useful in practice because companies may not have a full bug history,
while they probably do have history of code changes.

Outperforms models based on prior faults, 15-38% decrease in prediction errors @hassan2009.
However, @DAmbros2010 contradicts this statement.

#### Prior faults
Methods based on prior faults argue that files which in the past contained faults will
probably have more faults in the future.
It is shown that predictors based on previous bugs show better results than 
predictors based on code changes @rahman2011.

##### FixCache @kim2007
Maintains a fixed-size cache of files that are most likely to contain bugs.
Files are added to cache if it meets one of the locality criteria, which are:

* Churn locality: if a file is recently modified it is likely to contain faults
* Temporal locality: if a file contains a fault, it is more likely to contain more faults
* Spatial locality: files that change alongside faulty files are more likely to contain faults

If the cache if full the least recently used file is removed from the cache.
The size of the cache is set at 10% of all files.
Hit rate of about 73-95% at file level, 46-72% at method level.

##### Rahman @rahman2011
The Rahman algorithm is based on the FixCache algorithm,
in their research they found that the temporal locality was by far the most influential factor.
The Rahman algorithm is thus implemented based solely on that factor.

##### Time-weighted risk algorithm @Lewis2013
An algorithm based on the Rahman algorithm,
it includes a weight factor based on how old a commit is, 
older bug fixing commits have less impact on the overall bug-proneness of the file.
Instead of showing top 10% of bug-prone files, this shows the top 20 files.

#### Other methods
Besides the more obvious reasonings listed above there are some other possible methods for bug
prediction.

##### Developer centered
@DiNucci2018 proposed a method based on the scattering of changes made by a developer.
These scattering is defined by:

* Structural scattering: based on amount of different components worked on (e.g. packages)
* Semantic scattering: based on the textual difference between classes 
  (i.e. classes which have similar behaviour are textually similar)

Developer experience has no clear correlation with how much faults they introduce @rahman2011.

##### Machine learning
With machine learning methods bugs can be predicted by software tools,
these tools train on the code base, code history and other logs.

Machine learning is also useful for reducing the amount of metrics used for 
bug prediction @shivaji2009.

Downside is that these methods give little insights in why a component is bug-prone @Lewis2013.

Another downside is that this doesn't work for new projects since cross-project
defect prediction does not yet give good results @zimmermann2009.

@Shepperd2014

##### Analysis on Abstract Syntax Trees
Because ASTs give a more high-level description of the syntax of a program
it could give more useful insights for bug-prediction than other analysis methods.

@wang2016 uses ASTs in combination with a machine learning approach for bug prediction.
They also mention that using features from ASTs improves cross-project prediction.

Downside is that ASTs is that dynamic programming languages cannot produce static ASTs.





### Evaluations

In order to answer this questions we looked at the scientific papers that 
benchmark prediction models in any way, 
and specifically looked at the way this are evaluated. 
We found that most papers use a simple yet logical approach to 
benchmark an algorithm’s performance. 
This approach is called Area Under the Curve (commonly referred as AUC), 
it consists of measuring the area under a curve 
which is known as Receiver Operating Characteristic (ROC).
This process graphically draws the relationship between the accuracy, 
which is the ability of an algorithm to find all existing bug-prone files, and precision, 
which is how accurate the algorithm is at finding them (optimal would be no false negatives). 
This approach is used in this papers: 
@DAmbros2012, @Arisholm2010, @Jiang2008, @Malhotra2015, @Lessman2008. 

Other authors use different methodologies in order to rank the algorithm’s performance. 
@Jiang2008 focuses solely on how to evaluate the algorithm’s performance, 
and the researchers used basically the accuracy and 
precision metrics in numerical and graphical form, 
but they also bring to the table a graphic that helps identify where to spend 
the project resources in order to get a greater software quality.
Finally, we came across @DAmbros2010, in which the authors choose to use 
linear regression in order to qualify the fault-prediction method’s prediction power.

In conclusion, we can see that the most used methodology is AUC, 
this is because, citing @Lessman2008: “The AUC was recommended as the primary accuracy indicator 
for comparative studies in software defect prediction since it separates predictive performance 
from class and cost distributions, which are project-specific characteristics that may be unknown 
or subject to change.”

It’s also worth noting the interesting conclusion of @Shepperd2014, 
in which they found that 30 percent of the variance of the algorithm’s performance 
can be “explained” based on the research group, 
while the variability due to the choice of classifier is very small.




### Practical usage
No significant effect on developers @Lewis2013.
Managers can use bug-prone list for quality assurance @kim2007.

#### Practical problems

* Obvious reasoning @Lewis2013
* Bias towards the new @Lewis2013
* Actionable messages @Lewis2013
* Noise problem @Kim2011
* Granularity: Most methods we found work on a class- or file-based granularity, 
  having a lower level of granularity could improve the usefulness of the prediction for 
  developers @giger2012
* Scalability @Lewis2013



### Actionable tools for developers

The only paper that tackles this question is @Lewis2013, 
in which they deploy bug predicting software to developers 
workspaces and evaluate developers behaviour after and before having the software. 
In this study they conclude currently, 
fault-prediction does not provide actionable tools for developers.

To answer this questions we found no more information in scientific papers, 
so we had to look outside of the academic world in order to see if this techniques are 
being used by developers. 
After some research we concluded that bug-prediction is not currently being used on developers, 
and we, as in @Lewis2013, think that maybe bug-prediction is supposed to be used for software 
quality in order to find the places where we should invest the project resources instead 
of helping developers write code without bugs.

### Open challenges and future work

* The role of developer-related factors in the bug prediction field 
  is still a partially explored area
* Still no strong conclusions - process metrics were found to perform better, but are more 
  expensive to obtain. Then again, code metrics have high stasis and might result in stale models. 
* Defect prediction is a field where external validity is very hard to achieve
